{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"16wKOlwhx9lr8IvCqM4EXw-4007s713XS","timestamp":1698521950497}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Implementation of **Word2Vec Algorithm using Skip Gram Model**\n","\n","[Reference text](https://keras.io/examples/nlp/)\n","\n","[Google Colab content](https://colab.research.google.com/drive/16wKOlwhx9lr8IvCqM4EXw-4007s713XS?usp=sharing)\n","\n","\n","References : [Main Source](https://analyticsindiamag.com/guide-to-word2vec-using-skip-gram-model/)"],"metadata":{"id":"ZiV4o4J9EEvK"}},{"cell_type":"markdown","source":["### The **word2vec** algorithm uses a neural network model to learn word associations from a large corpus of text.      \n","\n","The continuous skip-gram model learns by predicting the surrounding words given a current word. In other words, the Continuous Skip-Gram Model predicts words within a certain range before and after the current word in the same sentence.\n","\n","The Skip-Gram model is trained on n-gram pairs of (target_word, context_word) with a token as 1 and 0. The token specifies whether the context_words are from the same window or generated randomly. The pair with token 0 is neglected.   "],"metadata":{"id":"S5RjvZAlD9MN"}},{"cell_type":"markdown","source":["**Code Implementation of Skip-Gram Model**\n","\n","Steps to be followed:\n","\n","Build the corpus vocabulary\n","\n","Build a skip-gram [(target, context), relevancy] generator\n","\n","Build the skip-gram model architecture\n","\n","Train the Model\n","Get Word Embeddings"],"metadata":{"id":"kKR6gCHuE_UE"}},{"cell_type":"markdown","source":["# 1. **Build the corpus vocabulary:**\n","\n","\n","The essential step while building any NLP based model is to create a corpus in which we extract each unique word from vocabulary and assign a unique numeric identifier to it.\n","\n","In this article, the corpus we are using is ‘The King James Version of the Bible’, from Project Gutenberg, available free through the corpus model in nltk.\n","\n","Import all dependencies:"],"metadata":{"id":"mgyx69CRFIPb"}},{"cell_type":"code","metadata":{"id":"EIEsHOvs5naD"},"source":["from nltk.corpus import gutenberg # to get bible corpus\n","from string import punctuation # to remove punctuation from corpus\n","import nltk\n","import re\n","import numpy as np\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TensorFlow** is an open-sourced end-to-end platform, a library for multiple machine learning tasks, while **Keras** is a high-level neural network library that runs on top of TensorFlow. Both provide high-level APIs used for easily building and training models, but Keras is more user-friendly because it's built-in Python."],"metadata":{"id":"vf6LMeMG0b6f"}},{"cell_type":"markdown","source":["**Keras** is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research. [https://keras.io/about]\n","\n","The development team states that Keras is:\n","\n","Simple — but not simplistic. Keras reduces developer cognitive load to free you to focus on the parts of the problem that matter.\n","Flexible — Keras adopts the principle of progressive disclosure of complexity: simple workflows should be quick and easy, while arbitrarily advanced workflows should be possible via a clear path that builds upon what you’ve already learned.\n","Powerful — Keras provides industry-strength performance and scalability: it is used by organizations and companies including NASA, YouTube, and Waymo."],"metadata":{"id":"kchzbQ0o0z84"}},{"cell_type":"markdown","source":["Open https://colab.research.google.com/ and register for a free account\n","\n","Create a new notebook within Colab\n","\n","Select Runtime from the menu and Change the runtime type\n","\n","Choose GPU from the Hardware accelerator options -\n","\n","click save"],"metadata":{"id":"5X-45hts1DJe"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILm3z-5r2gnN","executionInfo":{"status":"ok","timestamp":1698522872887,"user_tz":-180,"elapsed":4282,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"f2a17601-0604-4814-b7e5-c89da31848a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"]}]},{"cell_type":"markdown","source":["**import TensorFlow**\n","\n","check that everything is set with the following few lines of code\n"],"metadata":{"id":"HXpxPpID1R6K"}},{"cell_type":"code","source":["# import TensorFlow\n","import tensorflow as tf\n","\n","#Check the version of TensorFlow you are using\n","print(tf.__version__)\n","print(tf.config.list_physical_devices('GPU'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wh8rGbUv1XK0","executionInfo":{"status":"ok","timestamp":1698522876022,"user_tz":-180,"elapsed":4,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"361c3bc8-4d64-4e1d-e242-47c1244e81e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.14.0\n","[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"markdown","source":["We are set , where we tested the tensorflow, its version and made sure it is connected to GPU"],"metadata":{"id":"c0-rqGxw1eNJ"}},{"cell_type":"code","source":["!pip install -q keras"],"metadata":{"id":"Rwz-zMz42OG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras"],"metadata":{"id":"EzXyFTbE1wng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the Sequential model class from Keras\n","# to form the framework for a Sequential neural network:\n","from keras.models import Sequential"],"metadata":{"id":"gCl0q5rR20-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from tensorflow.keras.layers import Activation, Dense\n","# tensorflow 2.4.0, the Dense move from keras.layers.core move to tensroflow.keras.layers\n","# from keras.layers.core import Dense\n","\n","#Please note if any module error comes, you need to check and find the right fix from Google!\n","#Many packages has shifted, if you are using old version, then it might give you different errors"],"metadata":{"id":"OEKQdg2J3tyG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing import text\n","from keras.layers import LSTM\n","from keras.preprocessing.sequence import skipgrams\n","from keras.layers import *\n","from tensorflow.keras.layers import Dense, Reshape,Activation\n","from tensorflow.keras.layers import Embedding\n","from keras.models import Model,Sequential"],"metadata":{"id":"CONJmZG50RMc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download gutenberg project, punkt model and stopwords from nltk as below:"],"metadata":{"id":"YXQZos8GFSqv"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkCExnM052Hq","executionInfo":{"status":"ok","timestamp":1698523641239,"user_tz":-180,"elapsed":1065,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"130855a3-25e9-4c23-a6a1-b40d6a25499b"},"source":["nltk.download('gutenberg')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = nltk.corpus.stopwords.words('english')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["A user-defined function for text preprocessing that removes extra whitespaces, digits, and stopwords and lower casing the text corpus."],"metadata":{"id":"nKZIltBAFZRc"}},{"cell_type":"code","metadata":{"id":"Vvd_fiR652F6"},"source":["bible = gutenberg.sents(\"bible-kjv.txt\")\n","remove_terms = punctuation + '0123456789'\n","#removes all punctuation and digits from it"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PoQuEDtcv0I","executionInfo":{"status":"ok","timestamp":1698523664506,"user_tz":-180,"elapsed":534,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"1d46259a-4d39-435d-8bf4-bbb91a769371"},"source":["bible"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['[', 'The', 'King', 'James', 'Bible', ']'], ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'], ...]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"oiU4yXEncrJ5"},"source":["wpt = nltk.WordPunctTokenizer()\n","\n","def normalize_document(doc):\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc,re.I|re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = wpt.tokenize(doc)\n","    # filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","normalize_corpus = np.vectorize(normalize_document)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define a vectorized function which takes a nested sequence of objects or numpy arrays as inputs and returns a single numpy array or a tuple of numpy arrays."],"metadata":{"id":"-GJ67dYY6cVn"}},{"cell_type":"markdown","source":["Next, to extract unique word from the corpus and assigning a numeric identifier."],"metadata":{"id":"A-1gWo3RFvFU"}},{"cell_type":"code","metadata":{"id":"mDAX-MAR52Ds"},"source":["norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n","norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n","norm_bible = filter(None, normalize_corpus(norm_bible))\n","norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvP43P_m511a","executionInfo":{"status":"ok","timestamp":1698524205204,"user_tz":-180,"elapsed":1587,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"abdcad5b-b285-4395-fd67-73aa93761cac"},"source":["tokenizer = text.Tokenizer()\n","tokenizer.fit_on_texts(norm_bible)\n","\n","word2id = tokenizer.word_index\n","id2word = {v:k for k, v in word2id.items()}\n","\n","vocab_size = len(word2id) + 1\n","\n","\n","wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n","print('Vocabulary Size:', vocab_size)\n","print('Vocabulary Sample:', list(word2id.items())[:5])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 12425\n","Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5)]\n"]}]},{"cell_type":"markdown","source":["2. **Build a Skip-Gram** [(target, context), relevancy] generator:\n","\n","Keras functional API provides model skip-gram, which generate a sequence of word index into tuples of words of the form:\n","\n","\n","(word, word in the same window), with label 1 (positive samples)\n","\n","\n","(word, random word from the vocabulary), with label 0 (negative samples)"],"metadata":{"id":"8qybSU2LF6Dn"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMADbZwT51zW","executionInfo":{"status":"ok","timestamp":1698524249241,"user_tz":-180,"elapsed":26765,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"c6a37ebc-44d0-4a05-9a90-296e65c86390"},"source":["# generate skip-grams\n","skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n","\n","# view sample skip-grams\n","pairs, labels = skip_grams[0][0], skip_grams[0][1]\n","for i in range(10):\n","    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n","          id2word[pairs[i][0]], pairs[i][0],\n","          id2word[pairs[i][1]], pairs[i][1],\n","          labels[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(king (13), bible (5766)) -> 1\n","(king (13), james (1154)) -> 1\n","(king (13), pleasantness (10581)) -> 0\n","(bible (5766), unwalled (5976)) -> 0\n","(james (1154), blastus (11713)) -> 0\n","(james (1154), bible (5766)) -> 1\n","(bible (5766), king (13)) -> 1\n","(james (1154), village (3416)) -> 0\n","(bible (5766), wherewithal (7791)) -> 0\n","(james (1154), king (13)) -> 1\n"]}]},{"cell_type":"markdown","source":["3. **Build the Skip-Gram model architecture:**\n","\n","By using Keras with backend support of TensorFlow, we will build a deep learning architect of skip-gram.\n","\n","Our input is targeted words, and context word pair means we need to process two inputs. This input is passed to a separate embedding layer to get word embedding for target and context words.\n","\n"," Afterwards, we combine these two layers and pass the result to a dense layer that predicts either 1 or 0 depending on whether a pair of words is contextually relevant or just randomly generated."],"metadata":{"id":"uXpppN8FGI_z"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4kAxBZzo51yS","executionInfo":{"status":"ok","timestamp":1698524417724,"user_tz":-180,"elapsed":3489,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"cc48541c-aa5b-4eaa-b26e-7a36482f244f"},"source":["# build skip-gram architecture\n","embed_size = 100\n","word_model = Sequential()\n","word_model.add(Embedding(vocab_size, embed_size,\n","                         embeddings_initializer=\"glorot_uniform\",\n","                         input_length=1))\n","word_model.add(Reshape((embed_size, )))\n","\n","context_model = Sequential()\n","context_model.add(Embedding(vocab_size, embed_size,\n","                  embeddings_initializer=\"glorot_uniform\",\n","                  input_length=1))\n","context_model.add(Reshape((embed_size,)))\n","\n","merged_output = add([word_model.output, context_model.output])\n","\n","model_combined = Sequential()\n","model_combined.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n","\n","final_model = Model([word_model.input, context_model.input], model_combined(merged_output))\n","final_model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n","\n","final_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," embedding_input (InputLaye  [(None, 1)]                  0         []                            \n"," r)                                                                                               \n","                                                                                                  \n"," embedding_1_input (InputLa  [(None, 1)]                  0         []                            \n"," yer)                                                                                             \n","                                                                                                  \n"," embedding (Embedding)       (None, 1, 100)               1242500   ['embedding_input[0][0]']     \n","                                                                                                  \n"," embedding_1 (Embedding)     (None, 1, 100)               1242500   ['embedding_1_input[0][0]']   \n","                                                                                                  \n"," reshape (Reshape)           (None, 100)                  0         ['embedding[0][0]']           \n","                                                                                                  \n"," reshape_1 (Reshape)         (None, 100)                  0         ['embedding_1[0][0]']         \n","                                                                                                  \n"," add (Add)                   (None, 100)                  0         ['reshape[0][0]',             \n","                                                                     'reshape_1[0][0]']           \n","                                                                                                  \n"," sequential_2 (Sequential)   (None, 1)                    101       ['add[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 2485101 (9.48 MB)\n","Trainable params: 2485101 (9.48 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":709},"id":"xqE6krKluvev","executionInfo":{"status":"ok","timestamp":1698524521271,"user_tz":-180,"elapsed":580,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"bcc2018d-2fd7-4411-b5fe-6192d203570a"},"source":["# visualize model structure\n","from IPython.display import SVG\n","#from keras.utils.vis_utils import model_to_dot\n","from tensorflow.keras.utils import plot_model,model_to_dot\n","\n","SVG(model_to_dot(final_model, show_shapes=True, show_layer_names=False,\n","                 rankdir='TB').create(prog='dot', format='svg'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"653pt\" height=\"516pt\" viewBox=\"0.00 0.00 490.00 387.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.75 0.75) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-383 486,-383 486,4 -4,4\"/>\n<!-- 134490533816944 -->\n<g id=\"node1\" class=\"node\">\n<title>134490533816944</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"11,-332.5 11,-378.5 221,-378.5 221,-332.5 11,-332.5\"/>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-351.8\" font-family=\"Times,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88,-332.5 88,-378.5 \"/>\n<text text-anchor=\"middle\" x=\"115.5\" y=\"-363.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88,-355.5 143,-355.5 \"/>\n<text text-anchor=\"middle\" x=\"115.5\" y=\"-340.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143,-332.5 143,-378.5 \"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-363.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 1)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143,-355.5 221,-355.5 \"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-340.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 1)]</text>\n</g>\n<!-- 134488554059568 -->\n<g id=\"node3\" class=\"node\">\n<title>134488554059568</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-249.5 0,-295.5 232,-295.5 232,-249.5 0,-249.5\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-268.8\" font-family=\"Times,serif\" font-size=\"14.00\">Embedding</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-249.5 80,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-272.5 135,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-249.5 135,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-272.5 232,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1, 100)</text>\n</g>\n<!-- 134490533816944&#45;&gt;134488554059568 -->\n<g id=\"edge1\" class=\"edge\">\n<title>134490533816944-&gt;134488554059568</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116,-332.37C116,-324.15 116,-314.66 116,-305.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-305.61 116,-295.61 112.5,-305.61 119.5,-305.61\"/>\n</g>\n<!-- 134487736146992 -->\n<g id=\"node2\" class=\"node\">\n<title>134487736146992</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"261,-332.5 261,-378.5 471,-378.5 471,-332.5 261,-332.5\"/>\n<text text-anchor=\"middle\" x=\"299.5\" y=\"-351.8\" font-family=\"Times,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"338,-332.5 338,-378.5 \"/>\n<text text-anchor=\"middle\" x=\"365.5\" y=\"-363.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"338,-355.5 393,-355.5 \"/>\n<text text-anchor=\"middle\" x=\"365.5\" y=\"-340.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"393,-332.5 393,-378.5 \"/>\n<text text-anchor=\"middle\" x=\"432\" y=\"-363.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 1)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"393,-355.5 471,-355.5 \"/>\n<text text-anchor=\"middle\" x=\"432\" y=\"-340.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 1)]</text>\n</g>\n<!-- 134487736146752 -->\n<g id=\"node4\" class=\"node\">\n<title>134487736146752</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"250,-249.5 250,-295.5 482,-295.5 482,-249.5 250,-249.5\"/>\n<text text-anchor=\"middle\" x=\"290\" y=\"-268.8\" font-family=\"Times,serif\" font-size=\"14.00\">Embedding</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"330,-249.5 330,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"357.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"330,-272.5 385,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"357.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"385,-249.5 385,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"433.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"385,-272.5 482,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"433.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1, 100)</text>\n</g>\n<!-- 134487736146992&#45;&gt;134487736146752 -->\n<g id=\"edge2\" class=\"edge\">\n<title>134487736146992-&gt;134487736146752</title>\n<path fill=\"none\" stroke=\"black\" d=\"M366,-332.37C366,-324.15 366,-314.66 366,-305.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"369.5,-305.61 366,-295.61 362.5,-305.61 369.5,-305.61\"/>\n</g>\n<!-- 134490677936576 -->\n<g id=\"node5\" class=\"node\">\n<title>134490677936576</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"18,-166.5 18,-212.5 232,-212.5 232,-166.5 18,-166.5\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\">Reshape</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-166.5 80,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-189.5 135,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-166.5 135,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-189.5 232,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n</g>\n<!-- 134488554059568&#45;&gt;134490677936576 -->\n<g id=\"edge3\" class=\"edge\">\n<title>134488554059568-&gt;134490677936576</title>\n<path fill=\"none\" stroke=\"black\" d=\"M118.46,-249.37C119.37,-241.15 120.43,-231.66 121.42,-222.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"124.92,-222.93 122.54,-212.61 117.96,-222.16 124.92,-222.93\"/>\n</g>\n<!-- 134487736148816 -->\n<g id=\"node6\" class=\"node\">\n<title>134487736148816</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"254,-166.5 254,-212.5 468,-212.5 468,-166.5 254,-166.5\"/>\n<text text-anchor=\"middle\" x=\"285\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\">Reshape</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"316,-166.5 316,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"343.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"316,-189.5 371,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"343.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"371,-166.5 371,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"419.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"371,-189.5 468,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"419.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n</g>\n<!-- 134487736146752&#45;&gt;134487736148816 -->\n<g id=\"edge4\" class=\"edge\">\n<title>134487736146752-&gt;134487736148816</title>\n<path fill=\"none\" stroke=\"black\" d=\"M364.63,-249.37C364.13,-241.15 363.54,-231.66 362.99,-222.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"366.47,-222.37 362.36,-212.61 359.49,-222.8 366.47,-222.37\"/>\n</g>\n<!-- 134487736151168 -->\n<g id=\"node7\" class=\"node\">\n<title>134487736151168</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"111,-83.5 111,-129.5 371,-129.5 371,-83.5 111,-83.5\"/>\n<text text-anchor=\"middle\" x=\"131\" y=\"-102.8\" font-family=\"Times,serif\" font-size=\"14.00\">Add</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"151,-83.5 151,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"178.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"151,-106.5 206,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"178.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"206,-83.5 206,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"288.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 100), (None, 100)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"206,-106.5 371,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"288.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n</g>\n<!-- 134490677936576&#45;&gt;134487736151168 -->\n<g id=\"edge5\" class=\"edge\">\n<title>134490677936576-&gt;134487736151168</title>\n<path fill=\"none\" stroke=\"black\" d=\"M156.7,-166.37C170.38,-156.81 186.54,-145.53 201.03,-135.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203.15,-138.2 209.34,-129.61 199.14,-132.46 203.15,-138.2\"/>\n</g>\n<!-- 134487736148816&#45;&gt;134487736151168 -->\n<g id=\"edge6\" class=\"edge\">\n<title>134487736148816-&gt;134487736151168</title>\n<path fill=\"none\" stroke=\"black\" d=\"M328.21,-166.37C314.06,-156.81 297.34,-145.53 282.35,-135.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"284,-132.3 273.75,-129.61 280.08,-138.1 284,-132.3\"/>\n</g>\n<!-- 134487736153136 -->\n<g id=\"node8\" class=\"node\">\n<title>134487736153136</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"135,-0.5 135,-46.5 347,-46.5 347,-0.5 135,-0.5\"/>\n<text text-anchor=\"middle\" x=\"172\" y=\"-19.8\" font-family=\"Times,serif\" font-size=\"14.00\">Sequential</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"209,-0.5 209,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"236.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"209,-23.5 264,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"236.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"264,-0.5 264,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"305.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"264,-23.5 347,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"305.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1)</text>\n</g>\n<!-- 134487736151168&#45;&gt;134487736153136 -->\n<g id=\"edge7\" class=\"edge\">\n<title>134487736151168-&gt;134487736153136</title>\n<path fill=\"none\" stroke=\"black\" d=\"M241,-83.37C241,-75.15 241,-65.66 241,-56.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"244.5,-56.61 241,-46.61 237.5,-56.61 244.5,-56.61\"/>\n</g>\n</g>\n</svg>"},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["**4.Train the model:**\n","\n","Training the model on a complete corpus takes more time; hence, we run a model for five epochs; you can increase the epochs if needed."],"metadata":{"id":"QTE3XxM4GXw9"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPywp81h51vq","executionInfo":{"status":"ok","timestamp":1698525092238,"user_tz":-180,"elapsed":548501,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"775516d5-9109-4821-9c17-a7352a3c3851"},"source":["for epoch in range(1, 3):\n","    loss = 0\n","    for i, elem in enumerate(skip_grams):\n","        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n","        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n","        labels = np.array(elem[1], dtype='int32')\n","        X = [pair_first_elem, pair_second_elem]\n","        Y = labels\n","        if i % 10000 == 0:\n","            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n","        loss += final_model.train_on_batch(X,Y)\n","\n","    print('Epoch:', epoch, 'Loss:', loss)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed 0 (skip_first, skip_second, relevance) pairs\n","Processed 10000 (skip_first, skip_second, relevance) pairs\n","Processed 20000 (skip_first, skip_second, relevance) pairs\n","Epoch: 1 Loss: 3462.401378194336\n","Processed 0 (skip_first, skip_second, relevance) pairs\n","Processed 10000 (skip_first, skip_second, relevance) pairs\n","Processed 20000 (skip_first, skip_second, relevance) pairs\n","Epoch: 2 Loss: 3345.8242130381986\n"]}]},{"cell_type":"markdown","source":["**5. Get word embeddings:**\n","\n","To get word embeddings for our entire vocabulary, we can extract the same from our embedding layer. We will extract the weights of embeddings from our word_model embedding layer."],"metadata":{"id":"6jPFyjojGhnX"}},{"cell_type":"code","metadata":{"id":"D8WawBoc9F-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698528029344,"user_tz":-180,"elapsed":1589,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"86b542cd-b250-47e8-f822-a6bed24be56e"},"source":["from sklearn.metrics.pairwise import euclidean_distances\n","word_embed_layer = word_model.layers[0]\n","weights = word_embed_layer.get_weights()[0][1:]\n","\n","distance_matrix = euclidean_distances(weights)\n","print(distance_matrix.shape)\n","\n","similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n","                   for search_term in ['god', 'jesus','egypt', 'john', 'famine']}\n","\n","similar_words"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(12424, 12424)\n"]},{"output_type":"execute_result","data":{"text/plain":["{'god': ['confirm', 'disperse', 'strive', 'zophah', 'behaviour'],\n"," 'jesus': ['submitted', 'jared', 'changing', 'blasphemest', 'illuminated'],\n"," 'egypt': ['revenue', 'gins', 'grecians', 'stairs', 'fellows'],\n"," 'john': ['mountains', 'eating', 'runneth', 'jehu', 'reserved'],\n"," 'famine': ['teraphim', 'bondmen', 'person', 'stretcheth', 'rekem']}"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["# **Interpretation:**\n","We can see that the model gives nearly correct words related to the target word. Accuracy can be increased by training more epochs but note it will add more computational time"],"metadata":{"id":"frmgcpoYGoMd"}},{"cell_type":"code","source":["! pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIHZZh32KCeJ","executionInfo":{"status":"ok","timestamp":1698527987881,"user_tz":-180,"elapsed":5242,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"07e6a201-aa2b-4d78-cbd3-a20febfb4d24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MrW0Yg-bMHih"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"2nqfdf-pD7yI"}},{"cell_type":"code","metadata":{"id":"Hh6ia6mC9TcB"},"source":[],"execution_count":null,"outputs":[]}]}