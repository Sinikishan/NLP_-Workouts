{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyo7u1gF4tPZrDyM5bqSjY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rnzn9CTC1Le7","executionInfo":{"status":"ok","timestamp":1698808040828,"user_tz":-180,"elapsed":8830,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"b45e315b-a9f9-4b17-8946-d618f317ed69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sklearn\n","  Using cached sklearn-0.0.post10.tar.gz (3.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]}],"source":["!pip install sklearn\n","!pip install nltk\n","import nltk"]},{"cell_type":"markdown","source":["The warning..\n","\n","× python setup.py egg_info did not run successfully.\n","  │ exit code: 1\n","  ╰─> See above for output.\n","\n","egg_info is package metadata. It stores data about your package, like its name, version, and dependencies.\n"],"metadata":{"id":"tIUhkFto2me2"}},{"cell_type":"markdown","source":["**1. Load the dataset**\n","\n","First, we need to load the dataset that we will use to train our text classification model. In this tutorial, we will use the 20 newsgroups dataset available in the sklearn library."],"metadata":{"id":"_GaJgD7y2y40"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","\n","newsgroups_data = fetch_20newsgroups(subset=\"train\")\n","X, y = newsgroups_data.data, newsgroups_data.target"],"metadata":{"id":"Vdi3kv6f2OEo","executionInfo":{"status":"ok","timestamp":1698808017451,"user_tz":-180,"elapsed":13439,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**2. Preprocess the text data**\n","\n","Text preprocessing is an essential step in NLP. It helps to clean and standardize the text data, making it easier for the machine learning model to understand and process the input. In this tutorial, we will use the nltk library to preprocess the text data. The preprocessing steps include tokenization, stopword removal, and stemming."],"metadata":{"id":"DwRcc9Q238jW"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer"],"metadata":{"id":"rqYO6gdD3oSt","executionInfo":{"status":"ok","timestamp":1698808385141,"user_tz":-180,"elapsed":552,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmPbLgLX4IEd","executionInfo":{"status":"ok","timestamp":1698808521124,"user_tz":-180,"elapsed":1008,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"6311ab0d-41b6-4bd6-f51b-a469b9859f98"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["def preprocess_text(text):\n","    tokens = word_tokenize(text)\n","    stop_words = set(stopwords.words(\"english\"))\n","    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n","    stemmer = PorterStemmer()\n","    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n","    return \" \".join(stemmed_tokens)\n","\n","X_preprocessed = [preprocess_text(text) for text in X]"],"metadata":{"id":"k0LJEBwo4Olh","executionInfo":{"status":"ok","timestamp":1698808612518,"user_tz":-180,"elapsed":77436,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**3. Transform text data into numerical features**\n","\n","Machine learning models work with numerical data, so we need to convert our text data into numerical features. One common approach is to use the Term Frequency-Inverse Document Frequency (TF-IDF) method. The sklearn library provides a TfidfVectorizer class that can be used to transform the text data."],"metadata":{"id":"rm97SEHm91hK"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","X_transformed = vectorizer.fit_transform(X_preprocessed)"],"metadata":{"id":"gSt3JeC49xXE","executionInfo":{"status":"ok","timestamp":1698810023994,"user_tz":-180,"elapsed":4912,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**4. Train the text classification model**\n","\n","After the text data is preprocessed and transformed, we can train our text classification model. In this tutorial, we will use the Multinomial Naive Bayes classifier from the sklearn library."],"metadata":{"id":"jzWBSDhG-BRY"}},{"cell_type":"markdown","source":["**What is the Multinomial Naive Bayes algorithm?**\n","\n","Multinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output.\n","\n","The presence or absence of a feature does not affect the presence or absence of the other feature.\n","\n","https://www.upgrad.com/blog/multinomial-naive-bayes-explained/"],"metadata":{"id":"zZTp4ZkdCsIV"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","\n","classifier = MultinomialNB()\n","classifier.fit(X_transformed, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"SebOul2599nY","executionInfo":{"status":"ok","timestamp":1698810080042,"user_tz":-180,"elapsed":40,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"ff0012c3-5924-4d90-ec73-7473fc5f0f6b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["**5. Evaluate the text classification model**\n","\n","After training our text classification model, we need to evaluate its performance using a test dataset. We can use the fetch_20newsgroups function to load the test dataset and preprocess it. Then, we can use the predict method to make predictions and calculate the accuracy of our model."],"metadata":{"id":"hZUnzxC_DLET"}},{"cell_type":"code","source":["test_data = fetch_20newsgroups(subset=\"test\")\n","X_test, y_test = test_data.data, test_data.target\n","\n","X_test_preprocessed = [preprocess_text(text) for text in X_test]\n","X_test_transformed = vectorizer.transform(X_test_preprocessed)\n","\n","y_pred = classifier.predict(X_test_transformed)\n","\n","from sklearn.metrics import accuracy_score\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjY3qUv-DE_C","executionInfo":{"status":"ok","timestamp":1698811478114,"user_tz":-180,"elapsed":55975,"user":{"displayName":"Sini kishan","userId":"05056090304666447450"}},"outputId":"a803dd53-62ef-43a4-dc68-10b311c6d995"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8029739776951673\n"]}]},{"cell_type":"markdown","source":["References :\n","[link text](https://reintech.io/blog/how-to-create-a-text-classification-model-with-python)"],"metadata":{"id":"s8nhsVyiDXBv"}}]}